import {
  AbletonClip,
  type AbletonClipRawData,
  type AbletonNote,
  type CurveValue,
  type PianoRollMpePoint,
  type PianoRollNoteLike,
  quickNote,
  createCurveValue,
  cloneCurveValue,
  pianoRollNoteToAbletonNote,
  abletonNoteToPianoRollNote,
  scaleTransposeMPE,
} from "@agentcombine/music-types";

export {
  AbletonClip,
  quickNote,
  createCurveValue,
  cloneCurveValue,
  pianoRollNoteToAbletonNote,
  abletonNoteToPianoRollNote,
  scaleTransposeMPE,
};

export type {
  AbletonNote,
  AbletonClipRawData,
  CurveValue,
  PianoRollMpePoint,
  PianoRollNoteLike,
};

const ws = new WebSocket('ws://localhost:8080');
export const clipMap = new Map<string, AbletonClip>();

const clipMapToRawData = (): Record<string, AbletonClipRawData> => {
  const data: Record<string, AbletonClipRawData> = {}
  clipMap.forEach((clip, key) => {
    data[key] = {
      name: clip.name,
      duration: clip.duration,
      notes: clip.notes.map(note => ({
        pitch: note.pitch,
        duration: note.duration,
        velocity: note.velocity,
        offVelocity: note.offVelocity,
        probability: note.probability,
        position: note.position,
        isEnabled: note.isEnabled,
        noteId: note.noteId,
        velocityDeviation: note.velocityDeviation,
        pitchCurve: note.pitchCurve,
        pressureCurve: note.pressureCurve,
        timbreCurve: note.timbreCurve,
        metadata: note.metadata
      }))
    }
  })
  return data
}

export const createClipDataTsSource = (data?: Record<string, AbletonClipRawData>) => {
  const clipData = data ?? clipMapToRawData()
  return `/* AUTO-GENERATED FILE – DO NOT EDIT
 *
 * Generated by createClipDataTsSource() in src/io/abletonClips.ts.
 */
import type { AbletonClipRawData } from '@/io/abletonClips';

export const clipData: Record<string, AbletonClipRawData> = ${JSON.stringify(clipData, null, 2)} as const;
`
}

export const createClipDataTsDownloadCallback = (fileName = 'clipData.ts') => {
  return () => {
    if (typeof window === 'undefined') {
      console.warn('createClipDataTsDownloadCallback called outside browser context')
      return
    }
    const source = createClipDataTsSource()
    const blob = new Blob([source], { type: 'text/plain' })
    const url = URL.createObjectURL(blob)
    const a = document.createElement('a')
    a.href = url
    a.download = fileName
    a.click()
    URL.revokeObjectURL(url)
  }
}

export const createClipDataJsonSource = (data?: Record<string, AbletonClipRawData>) => {
  const clipData = data ?? clipMapToRawData()
  return JSON.stringify(clipData, null, 2)
}

export const createClipDataJsonDownloadCallback = (fileName = 'clipData.json') => {
  return () => {
    if (typeof window === 'undefined') {
      console.warn('createClipDataJsonDownloadCallback called outside browser context')
      return
    }
    const json = createClipDataJsonSource()
    const blob = new Blob([json], { type: 'application/json' })
    const url = URL.createObjectURL(blob)
    const a = document.createElement('a')
    a.href = url
    a.download = fileName
    a.click()
    URL.revokeObjectURL(url)
  }
}

export const loadClipDataFromJson = (jsonText: string) => {
  let parsed: Record<string, AbletonClipRawData> | null = null
  try {
    parsed = JSON.parse(jsonText)
  } catch (err) {
    console.warn('Failed to parse clip JSON', err)
    return false
  }
  if (!parsed || typeof parsed !== 'object') return false

  clipMap.clear()
  Object.entries(parsed).forEach(([key, value]) => {
    clipMap.set(key, new AbletonClip(value.name, value.duration, value.notes))
  })
  return true
}


ws.onopen = () => {
  console.log('Connected to server');
};

ws.onclose = () => {
  console.log('Disconnected from server');
};

//requires alsParsing.ts to be running
export async function INITIALIZE_ABLETON_CLIPS(fileName: string, staticClipData?: Record<string, AbletonClipRawData>, forceJson?: boolean) {

  // ── Production — use static data and return immediately ──────────────
  if (import.meta.env.PROD || forceJson) {
    if (!staticClipData) {
      console.warn(
        'INITIALIZE_ABLETON_CLIPS called in production without static clip data; clipMap left empty.'
      );
      return;
    }

    clipMap.clear();
    (Object.entries(staticClipData) as [string, AbletonClip][]).forEach(
      ([key, value]) =>
        clipMap.set(key, new AbletonClip(value.name, value.duration, value.notes))
    );
    console.log('Ableton clips initialised from static data →', clipMap);
    return;                        // synchronous in prod
  }


  const ABLETON_CLIPS_READY = new Promise<void>((resolve) => {

    ws.onmessage = (message) => {
      console.log(`Received message from server: ${message.data.slice(0, 100)}`);
      const mes = JSON.parse(message.data);
      if (mes.type === 'fresh_clipMap' || mes.type === 'clipMap') { //see alsParsing.ts for message types - fresh_clipMap indicates first load of a new file, clipMap is a hot reload
        clipMap.clear();
        Object.entries(mes.data).forEach(([key, value]: [string, AbletonClip]) => {
          const actualClip = new AbletonClip(value.name, value.duration, value.notes);
          console.log("clip updated for", key)
            clipMap.set(key, actualClip);
        });
        console.log('clipMap updated', clipMap);
        if (mes.type === 'fresh_clipMap') resolve()
      }
    };
    
  })

  ws.send(JSON.stringify({ type: 'file', fileName }))

  return ABLETON_CLIPS_READY
}




/*
different types of transformations (for now, of monophonic material)
- vertical stretch/squash (keeps same melodic contour)
    optionally stays in scale
    can optionally be centered on a note
- inversion
    optionally stays in scale
    can optionally be centered on a note
- retrograde
- be able to apply these transforms to a time slice of a clip
- lerp between two different melodic contours?
    create a multi-segment line from where the notes are,
    fill in the blank subdivisions with interpolated notes,
    then lerp, then drop the interpolated notes to retain original rhythm

*/


/*
add a quick way to take some clip manipulations and render them out 
to a midi file (so you can import them back to ableton). this also
requires changing/adding some data to the AbletonClip class, or adding
some "renderer" class that can be written to (which holds the data) before
being written to a midi file.

If you add a targetClip.writeNotes(startOffset, sourceClip) method, 
that could be enough? Would need to handle note overlaps (do the same way 
ableton does? or add other options?)
)
*/

/*
- already have a helper for abstract chords/scales
- and already have a helper for rendered notes
- need a helper for abstract rhythms
- between chords/scales + rhythms, can generate rendered melodies
- also might need a helper for synth param ramps/expressions
- also might need a melodic countour helper

with these building blocks, can start composing generative music
with "mid level" abstractions that are more concrete than rules
but less explicit than explicit melodic/harmonic fragments

*/
